---
title: "Visualizing JSON Schema"
author: "Jeremy Stanley"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\usepackage[utf8]{inputenc}
---

```{r, echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>", fig.width = 7, fig.height = 5)
options(dplyr.print_min = 4L, dplyr.print_max = 4L)
```

JSON is a very simple data standard that, through nested data structures, can
represent incredibly complex datasets. In some cases, a set of JSON data
closely corresponds to a table in a SQL database. However, more commonly a
JSON document more closely maps to an entire SQL databse.

Understanding the structure of your JSON data is critical before you begin
analyzing the data. In this vignette, we use `tidyjson` to inspect the
structure of JSON data and then create various visualizations to help
understand a complex JSON dataset.

## JSON Definition

For a refrehser, see the [JSON specification](http://www.json.org/), which is
a very concise summary of how JSON is formatted. In essence, there are
three types of JSON data structures.

Per the specification, an object is a name/value pair, like 
`'{"string": "value"}'`:

![](http://www.json.org/object.gif)

An array is an ordered list, like `'[1, 2, 3]'`:

![](http://www.json.org/array.gif)

A value is a string, number, logical or NULL scalar:

![](http://www.json.org/value.gif)

What is particularly interesting about JSON is the following:

* Documents can omit objects, and so no single document is representative of
the schema of a collection
* Objects and arrays can be deeply nested, it is not uncommon to have an 
object with arrays of objects with arrays of objects of values
* The system generating JSON can change over time, and so older documents can
have different implicit schemas from newer documents

All of this makes visualizing JSON data structures very useful when embarking
on an analysis.

## Load required libraries

Before we start, let's load `tidyjson` along with other data manipulation and
visualization libraries, and set a seed so we get consistent results.

```{r, message = FALSE}
library(needs)
needs(tidyjson, jsonlite, dplyr, purrr, magrittr, forcats,
      ggplot2, igraph, RColorBrewer, wordcloud, viridis)
set.seed(1)
```

## Companies Data

Let's work with the `companies` dataset included in the `tidyjson` package, 
originating at [jsonstudio](http://jsonstudio.com/resources). It is a 
`r class(companies)` vector of 
`r length(companies) %>% format(big.mark = ',')` 
JSON strings, each describing a startup company.

First, let's convert the JSON to a nested list using `jsonlite::fromJSON`, where
we use `simplifyVector = FALSE` to avoid any simplification (which, while handy
can lead to inconsistent results across documents which may not have the same
set of objects).

```{r}
co_list <- companies %>% map(fromJSON, simplifyVector = FALSE)
```

We can then find out how complex each record is by recursively unlisting it
and computing the length:

```{r}
co_length <- co_list %>% map(unlist, recursive = TRUE) %>% map_int(length)
```

Then we can visualize the distribution of lengths on a log-scale:

```{r}
co_length %>%
  data_frame(length = .) %>%
  ggplot(aes(length)) +
    geom_density() +
    scale_x_log10() +
    annotation_logticks(side = 'b')
```

It appears that some companies have unlisted length less than 10, while others 
are in the hundreds or even thousands. The median is `r median(co_length)`.

Let's pick an example that is particularly small to start with:

```{r}
first_examp_index <- co_length %>% 
  detect_index(equals, 20L)

co_examp <- companies[first_examp_index]

co_examp
```

Even for such a small example it's hard to understand the structure from the
raw JSON. We can instead use `jsonlite::prettify` to print a prettier version:

```{r}
co_examp %>% 
  prettify(indent = 2) %>% 
  capture.output %>%
  paste(collapse = "\n") %>%
  gsub("\\[\n\n( )*\\]", "[ ]", .) %>%
  writeLines
```

Where everything after `prettify` is done to collapse empty arrays from
occupying multiple lines, of which there are many.

Alternatively, we can visualize the same object after we converted it into
an R list using `str`:

```{r}
str(co_list[first_examp_index])
```

Alternatively, we can compute the structure using `tidyjson::json_structure`

```{r}
co_examp %>% json_structure %>% select(-document.id)
```

This gives us a `data.frame` where each row corresponds to an object, array
or scalar in the JSON document.

## Working with many companies

```{r}
structure <- companies %>% json_structure

keys <- structure %>% 
  filter(type != "null" & !is.na(key)) %>%
  group_by(level, key, type) %>%
  summarize(ndoc = n_distinct(document.id))

keys
```

As a word cloud

```{r}
keys %$% wordcloud(key, ndoc, scale = c(1.5, .1), min.freq = 100)
```

Alternatively in ggplot2

```{r, fig.height = 9}
keys %>%
  ungroup %>%
  group_by(type) %>%
  arrange(desc(ndoc), level) %>%
  mutate(rank = 1:n()) %>%
  ggplot(aes(1, rank)) +
    geom_text(aes(label = key, color = ndoc)) +
    scale_y_reverse() +
    facet_grid(. ~ type) +
    theme_void() +
    theme(legend.position = "bottom") +
    scale_color_viridis(direction = -1)
```

This shows there are many comon strings and arrays, and many uncommon 
objects, strings and numbers in the documents.

## Visualizing as Graphs

To really understand the structure of a document, we need to visaulize it as
a graph. This function will plot the structure of a document using `igraph`.

```{r}
plot_structure_graph <- function(json, legend = TRUE, vertex.size = 6,
                                 edge.width = .5, show.labels = TRUE) {
  
  structure <- json %>% json_structure
  
  type_colors <- brewer.pal(6, "Accent")
  
  graph_edges <- structure %>%
    filter(!is.na(parent.id)) %>%
    select(parent.id, child.id)
  
  graph_vertices <- structure %>% 
    transmute(child.id, 
              vertex.color = type_colors[as.integer(type)],
              vertex.label = key)
  
  if (!show.labels)
    graph_vertices$vertex.label <- rep(NA_character_, nrow(graph_vertices))

  g <- graph_from_data_frame(graph_edges, vertices = graph_vertices,
                             directed = FALSE)
  
  op <- par(mar = c(0, 0, 0, 0))
  plot(g, 
       vertex.color = V(g)$vertex.color, 
       vertex.size = vertex.size,
       vertex.label = V(g)$vertex.label, 
       vertex.frame.color = NA,
       layout = layout_with_kk, 
       edge.color = 'grey70', 
       edge.width = edge.width)
  
  if (legend)
    legend(x = -1.3, y = -.6, levels(structure$type), pch = 21,
           col= "white", pt.bg = type_colors,
           pt.cex = 2, cex = .8, bty = "n", ncol = 1)
  
  par(op)
  
  invisible(NULL)
  
}
```

Let's look at our simple example

```{r}
co_examp %>% plot_structure_graph
```

For this company, almost all of the data is in the top level object.

Now, let's create a function that plots a panel of these graphs:

```{r, fig.height = 8}
plot_structure_panel <- function(json, nrow, ncol, ...) {
  
  # Set up grid
  op <- par(mfrow = c(nrow, ncol))
  
  indices <- seq_along(json) %>% keep(`<=`, nrow * ncol)
  
  for (i in indices) {
    plot_structure_graph(json[[i]], ...)
    if ("names" %in% names(attributes(json))) 
      title(names(json)[i], col.main = 'red')
  }
  
  par(op)
  invisible(NULL)
}
```

And let's look at several simultaneously:

```{r, fig.height = 8}
plot_structure_panel(companies, 7, 6, legend = FALSE, show.labels = FALSE,
                     vertex.size = 4)
```

Clearly there is a huge amount of variety in the JSON documents!

Let's look at the most complex example:

```{r}
most_complex <- companies[which(co_length == max(co_length))]

most_complex_name <- most_complex %>% 
  spread_values(name = jstring("name")) %>% 
  extract2("name")
``` 

The most complex company is `r most_complex_name`! Let's try to plot it:

```{r}
plot_structure_graph(most_complex, show.labels = FALSE, vertex.size = 2)
```

That is just too big, let's simplify things by just looking at the top level
objects

```{r}
objects <- most_complex %>%
  gather_keys %>%
  json_types %>%
  filter(type == "object")

objects %>% 
  split(.$key) %>%
  plot_structure_panel(2, 2, legend = FALSE)
```

Now let's look at just the arrays, and take only the longest for each

```{r}
arrays <- most_complex %>%
  gather_keys %>%
  json_types %>%
  filter(type == "array") %>%
  gather_array

arrays %>%
  ggplot(aes(key)) +
    geom_bar() +
    coord_flip()
```

Many are very long, and likely have the similar structures (but no guarantees
in JSON!), so let's just look at the first for each:

```{r, fig.height = 9}
arrays %>%
  filter(array.index == 1) %>%
  split(.$key) %>%
  plot_structure_panel(4, 3, legend = FALSE)
```

## Working with funding data

Now let's use this insight to structure funding and geo data for a visualization.

First, let's get funding round data:

```{r}
rounds <- companies %>%
  enter_object("funding_rounds") %>%
  gather_array %>%
  spread_values(
    round = jstring("round_code"),
    currency = jstring("raised_currency_code"),
    raised = jnumber("raised_amount")
  )
rounds %>% head
```

Now, let's get geo data:

```{r}
geos <- companies %>%
  enter_object("offices") %>%
  gather_array %>%
  spread_values(
    country = jstring("country_code"),
    state = jstring("state_code"),
    description = jstring("description")
  )
geos %>% head
```

Now, let's join together the data for the US companies, assuming that the
first office in the array is the headquarters (seems reasonable from a quick
visual inspection of `geos`).

```{r}
hqs <- geos %>%
  filter(array.index == 1) %>%
  filter(country == "USA") %>%
  select(document.id, state)
  
rounds_usd <- rounds %>%
  filter(currency == "USD") %>%
  filter(!is.na(raised)) %>%
  select(document.id, round, raised)

rounds_by_geo <- inner_join(rounds_usd, hqs, by = "document.id")
```

Now we can visualize the results

```{r}
rounds_by_geo %>%
  filter(!is.na(state)) %>%
  mutate(
    round = round %>% fct_collapse(
      "angel" = c("seed", "angel"),
      "d-f"   = c("d", "e", "f"),
      "other" = c("grant", "partial", "post_ipo_equity", "private_equity", 
                  "debt_round", "unattributed")
    ) %>% fct_relevel("angel", "a", "b", "c", "d-f", "other")
  ) %>%
  mutate(
    state = state %>% fct_lump(2)
  ) %>%
  ggplot(aes(state, raised, fill = state)) +
    geom_violin() +
    scale_y_log10() + 
    annotation_logticks(side = 'l') +
    facet_grid(. ~ round) +
    theme(legend.position = "bottom") +
    labs(x = "", y = "Amount Raised (USD)")
```

Which shows a few interesting things:

* Round sizes don't increase dramatically from b through f
* Variance is much higher in angel and a rounds (and other)
* NY and other US states have similar distributions with the exception of d-f
rounds, where CA seems higher than NY
